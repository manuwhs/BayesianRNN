{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BACKGROUND: \n",
    "In VAE exercise for DL course, the variational free energy loss function (aka ELBO) has 2 terms: (1) the reconstruction\n",
    "error of the decoder, and (2) the KL divergence between our approximate posterior (aka z or latent distribution) and \n",
    "the prior we provide. \n",
    "\n",
    "\n",
    "CONNECTION WITH BAYES BY BACKPROP:\n",
    "Just as the VAE loss function optimizes for the parameters of the latent distribution (while keeping the reconstruction\n",
    "error at bay) the loss function in Blundell (Bayes by Backprop loss) optimizes for the parameters (\\mu, \\sigma) that\n",
    "define the weight distribution of our neural network (while keeping accuracy error at bay). \n",
    "\n",
    "Therefore, to implement Bayes by Backprop, we simply have to borrow the VAE loss function - this will allow us to \n",
    "approximate the true distribution of the neural network's weights. \n",
    "\n",
    "\n",
    "DATA BATCHING:\n",
    "If our training set contains M number of samples (i.e. sentences in our case) and B number of batches (called \n",
    "mini-batches in Fortunato et al 2017), then we will have C = M/B number of samples per batch. Further, let's assume\n",
    "that each sample/sentence contains 35 elements/words. \n",
    "\n",
    "BAYES BY BACKPROP THROUGH TIME:\n",
    "I think it's easier to visualize with the unrolled RNN. Let's say that we unroll for 35 time steps (T=35). What this\n",
    "means is that every word in a sentence corresponds to a time step (since the task is next-word prediction). Let's follow \n",
    "the path of a sample/sentence. Assume this sentence, \"The dinosaur likes to go dancing.\" The element \"The\" will go to\n",
    "LSTM_t1; the element \"dinosaur\" will go to LSTM_t2; and so on. \n",
    "\n",
    "I'M IN DOUBT HERE: At each timestep we will also have a y-output predicting what the next word will be in the form of\n",
    "a 10K softmax vector. This means that backpropagation will be computed per word. I.e., we will forward propagate a word\n",
    "through the network, make a next-word prediction, backpropagate given that prediction, and thus obtain gradients per \n",
    "word (and equivalently, per timestep). \n",
    "\n",
    "OPTIMIZATION:\n",
    "Our direction of descent will be the average of the gradients of the C samples per batch. The gradient of each sample,\n",
    "in turn, will be the average of the gradients of the 35 timesteps (i.e. the average of the 35 words). This weighting \n",
    "scheme is used by Fortunato; Blundell experiment with another that might yield better results. \n",
    "\n",
    "Either way, the main take-away is that in order to implement Bayes by Backprop through Time, we need to: \n",
    "    1. Figure out how to get gradients at the word (or timestep) level in tensorflow\n",
    "    2. Tell tensorflow how to weight/average these gradients per step direction.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#M = number of total samples in our training set \n",
    "batch_size = config.batch_size\n",
    "n_batches = M / float(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# computing cross entropy per sample\n",
    "def categorical_cross_entropy(p, t, eps=1e-10):\n",
    "    return -tf.reduce_sum(t * tf.log(p+eps), axis=[1])\n",
    "\n",
    "\n",
    "# computing kl divergence per sample or batch?\n",
    "def kl_normal2_stdnormal(mean, log_var, eps=0.0):\n",
    "    \"\"\"\n",
    "    Compute analytically integrated KL-divergence between a diagonal covariance Gaussian and \n",
    "    a standard Gaussian.\n",
    "\n",
    "    In the setting of the variational autoencoder, when a Gaussian prior and diagonal Gaussian \n",
    "    approximate posterior is used, this analytically integrated KL-divergence term yields a lower variance \n",
    "    estimate of the likelihood lower bound compared to computing the term by Monte Carlo approximation.\n",
    "\n",
    "        .. math:: D_{KL}[q_{\\phi}(z|x) || p_{\\theta}(z)]\n",
    "\n",
    "    See appendix B of [KINGMA]_ for details.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : Tensorflow tensor\n",
    "        Mean of the diagonal covariance Gaussian.\n",
    "    log_var : Tensorflow tensor\n",
    "        Log variance of the diagonal covariance Gaussian.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tensorflow tensor\n",
    "        Element-wise KL-divergence, this has to be summed when the Gaussian distributions are multi-variate.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "        ..  [KINGMA] Kingma, Diederik P., and Max Welling.\n",
    "            \"Auto-Encoding Variational Bayes.\"\n",
    "            arXiv preprint arXiv:1312.6114 (2013).\n",
    "\n",
    "    \"\"\"\n",
    "    return -0.5 * tf.reduce_sum(1 + log_var - tf.square(mean) - tf.exp(log_var), axis=1)\n",
    "\n",
    "\n",
    "c = - 0.5 * math.log(2*math.pi)\n",
    "def log_normal2(x, mean, log_var, eps=0.0):\n",
    "    \"\"\"\n",
    "    Compute log pdf of a Gaussian distribution with diagonal covariance, at values x.\n",
    "    Here variance is parameterized in the log domain, which ensures :math:`\\sigma > 0`.\n",
    "\n",
    "        .. math:: \\log p(x) = \\log \\mathcal{N}(x; \\mu, \\sigma^2I)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : Tensorflow tensor\n",
    "        Values at which to evaluate pdf.\n",
    "    mean : Tensorflow tensor\n",
    "        Mean of the Gaussian distribution.\n",
    "    log_var : Tensorflow tensor\n",
    "        Log variance of the diagonal covariance Gaussian.\n",
    "    eps : float\n",
    "        Small number used to avoid NaNs\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tensorflow tensor\n",
    "        Element-wise log probability, this has to be summed for multi-variate distributions.\n",
    "    \"\"\"\n",
    "    return tf.reduce_sum(c - log_var/2 - tf.square(x - mean) / (2 * tf.exp(log_var) + eps), axis=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eps = 1e-10\n",
    "\n",
    "# Loss Function Based on Blundell et al 2015 Equation 1\n",
    "\n",
    "#______ Version 1: Analytical Closed form solution _______#\n",
    "# Likelihood cost (accuracy error), log[p(D|w)] in [-\\infty, 0]). \n",
    "log_p_D_given_w = -tf.reduce_mean(categorical_cross_entropy(l_out, tf.expand_dims(x_pl,1), eps=eps), name = \"log_likelihood\")\n",
    "\n",
    "# Regularization cost: \n",
    "# Kulback-Leibler divergence between approximate posterior, q(w|\\theta)\n",
    "# and prior p(w)=N(w,mu,sigma*I). In this case we use a gaussian prior - not a mixture\n",
    "KL_qp = tf.reduce_mean(kl_normal2_stdnormal(l_mu_q, l_logvar_q, eps=eps), name = \"KL\")\n",
    "\n",
    "# Combining the two terms in the variational free energy (aka ELBO): Blundell Eq1   \n",
    "ELBO_loss = tf.subtract(KL_qp, log_p_x_given_z, name=\"ELBO\")\n",
    "\n",
    "\n",
    "#______ Version 2: Approximation via Monte Carlo _______#\n",
    "# This might be useful if we figure out how to implement with gaussian mixtures as priors\n",
    "#ELBO_loss = ((1. / n_batches) * (log_qw - log_pw) - log_likelihood).sum() / float(batch_size)\n",
    "\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.25)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "\n",
    "# Training operator for applying the loss gradients in backpropagation update\n",
    "train_op = optimizer.minimize(ELBO_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
